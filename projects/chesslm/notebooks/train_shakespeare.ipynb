{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Largely adopted from https://github.com/karpathy/build-nanogpt/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_URL = (\n",
    "    \"https://raw.githubusercontent.com/karpathy/char-rnn/master\"\n",
    "    \"/data/tinyshakespeare/input.txt\"\n",
    ")\n",
    "DATA_DIR = os.path.expanduser(\"~/Data/tinyshakespeare\")\n",
    "DATA_FILENAME = \"input.txt\"\n",
    "DATA_FILEPATH = os.path.join(DATA_DIR, DATA_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TEST_LINES = 8_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: mps\n"
     ]
    }
   ],
   "source": [
    "DEVICE = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    DEVICE = \"mps\"\n",
    "DEVICE_TYPE = \"cuda\" if DEVICE.startswith(\"cuda\") else \"cpu\"\n",
    "print(f\"using device: {DEVICE}\")\n",
    "\n",
    "\n",
    "SEED = 1337\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from urllib import request\n",
    "\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "if not os.path.isfile(DATA_FILEPATH):\n",
    "    content = request.urlopen(DATASET_URL)\n",
    "    with open(DATA_FILEPATH, \"wb\") as f:\n",
    "        f.write(content.read())\n",
    "\n",
    "with open(DATA_FILEPATH, \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(example := text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5962, 22307, 25, 198, 8421, 356, 5120, 597, 2252, 11, 3285, 502, 2740, 13, 198, 198, 3237, 25, 198, 5248, 461, 11, 2740, 13, 198]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "ENCODER = tiktoken.get_encoding(\"gpt2\")\n",
    "example_tokens = ENCODER.encode(example)\n",
    "print(example_tokens[:24 + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5962, 22307,    25,   198,  8421,   356],\n",
      "        [ 5120,   597,  2252,    11,  3285,   502],\n",
      "        [ 2740,    13,   198,   198,  3237,    25],\n",
      "        [  198,  5248,   461,    11,  2740,    13]])\n",
      "tensor([[22307,    25,   198,  8421,   356,  5120],\n",
      "        [  597,  2252,    11,  3285,   502,  2740],\n",
      "        [   13,   198,   198,  3237,    25,   198],\n",
      "        [ 5248,   461,    11,  2740,    13,   198]])\n"
     ]
    }
   ],
   "source": [
    "example_buf = torch.tensor(example_tokens[:24 + 1])\n",
    "example_x = example_buf[:-1].view(4, 6)\n",
    "example_y = example_buf[1:].view(4, 6)  # predict the next token\n",
    "print(example_x)\n",
    "print(example_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000\n"
     ]
    }
   ],
   "source": [
    "print(len(lines := text.splitlines()))\n",
    "train_text = \"\\n\".join(lines[:-N_TEST_LINES])\n",
    "val_text = \"\\n\".join(lines[-N_TEST_LINES:])\n",
    "del lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class DataLoaderLite:\n",
    "    # We are not doing DDP in this project whatsoever, and we're also only training on\n",
    "    # tinyshakespeare, so this is a significantly downsized version of the DataLoaderLite\n",
    "    # in the tutorial\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        B: int,\n",
    "        T: int,\n",
    "        text: str,\n",
    "        loop: bool = False,\n",
    "    ):\n",
    "        self.B: int = B\n",
    "        self.T: int = T\n",
    "        self.text: str = text\n",
    "        self.loop: bool = loop\n",
    "        self.reset()\n",
    "\n",
    "    @property\n",
    "    def BT(self) -> int:\n",
    "        return self.B * self.T\n",
    "\n",
    "    def reset(self):\n",
    "        self._buf = []\n",
    "        self._pos = 0\n",
    "\n",
    "    def next_tokens(self, add_pred_token: bool = False) -> list[int]:\n",
    "        BT = self.BT\n",
    "        n_tokens = BT + int(add_pred_token)\n",
    "        pos_step = n_tokens * 4\n",
    "\n",
    "        while len(self._buf) < n_tokens:\n",
    "            segment = self.text[self._pos : self._pos + pos_step]\n",
    "            if not(segment):\n",
    "                raise RuntimeError(\"no tokens remaining\")\n",
    "            if match := re.search(r\"\\W+\", segment[::-1]):\n",
    "                segment = segment[: len(segment) - match.end() + 1]\n",
    "            tokens = ENCODER.encode(segment)\n",
    "            self._buf.extend(tokens)\n",
    "            self._pos += len(segment)\n",
    "\n",
    "        tokens = self._buf[: n_tokens]  # we want this many tokens\n",
    "        self._buf = self._buf[BT :]  # remove BT tokens (not BT + 1!)\n",
    "        return tokens\n",
    "\n",
    "    def next_batch(self) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        batch_tokens = torch.as_tensor(self.next_tokens(add_pred_token=True))\n",
    "        try:\n",
    "            x = (batch_tokens[:-1]).view(self.B, self.T) # inputs\n",
    "            y = (batch_tokens[1:]).view(self.B, self.T) # targets\n",
    "        except RuntimeError:\n",
    "            raise RuntimeError(\"no more batches remaining\")\n",
    "        return x, y\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        try:\n",
    "            return self.next_batch()\n",
    "        except RuntimeError:\n",
    "            if self.loop:\n",
    "                self.reset()\n",
    "                try:\n",
    "                    return self.next_batch()\n",
    "                except RuntimeError:\n",
    "                    raise StopIteration\n",
    "            else:\n",
    "                raise StopIteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[43, 29625, 220, 2419, 388, 288, 45621, 1650, 716, 316, 11, 369, 8831]\n",
      "[8831, 316, 333, 31659, 271, 2259, 220, 417, 270, 11, 10081, 466, 304]\n",
      "[304, 3754, 4666, 10042, 753, 312, 312, 2797, 3384, 2248, 382, 2123, 220]\n",
      "[220, 67, 349, 382, 2153, 2616, 435, 1557, 64, 13, 7273, 551, 320]\n",
      "\n",
      "Lorem ipsum dolor sit amet, consect\n",
      "sectetur adipiscing elit, sed do e\n",
      " eiusmod tempor incididunt ut labore et \n",
      " dolore magna aliqua. Ut enim\n",
      "\n",
      "[43, 29625, 220, 2419, 388, 288, 45621, 1650, 716, 316, 11, 369]\n",
      "[8831, 316, 333, 220, 324, 541, 271, 2259, 1288, 270, 11, 10081]\n",
      "[466, 304, 3754, 4666, 10042, 220, 1939, 312, 312, 2797, 3384, 2248]\n",
      "[382, 2123, 288, 349, 382, 2153, 2616, 435, 1557, 64, 13, 7273]\n",
      "\n",
      "Lorem ipsum dolor sit amet, con\n",
      "sectetur adipiscing elit, sed\n",
      " do eiusmod tempor incididunt ut lab\n",
      "ore et dolore magna aliqua. Ut\n"
     ]
    }
   ],
   "source": [
    "example = (\n",
    "    \"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor\"\n",
    "    \" incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam,\"\n",
    ")\n",
    "dataloader = DataLoaderLite(2, 6, example)\n",
    "\n",
    "def print_next_tokens(add_pred_token: bool):\n",
    "    print(dataloader.next_tokens(add_pred_token=add_pred_token))\n",
    "\n",
    "def print_next_segment(add_pred_token: bool):\n",
    "    print(ENCODER.decode(dataloader.next_tokens(add_pred_token=add_pred_token)).replace(\"\\n\", \"\\\\n\"))\n",
    "\n",
    "dataloader.reset()\n",
    "print_next_tokens(True)\n",
    "print_next_tokens(True)\n",
    "print_next_tokens(True)\n",
    "print_next_tokens(True)\n",
    "print()\n",
    "dataloader.reset()\n",
    "print_next_segment(True)\n",
    "print_next_segment(True)\n",
    "print_next_segment(True)\n",
    "print_next_segment(True)\n",
    "print()\n",
    "dataloader.reset()\n",
    "print_next_tokens(False)\n",
    "print_next_tokens(False)\n",
    "print_next_tokens(False)\n",
    "print_next_tokens(False)\n",
    "print()\n",
    "dataloader.reset()\n",
    "print_next_segment(False)\n",
    "print_next_segment(False)\n",
    "print_next_segment(False)\n",
    "print_next_segment(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class CosineLRSchedule(BaseModel):\n",
    "    max_lr: float\n",
    "    min_lr: float\n",
    "    warmup_steps: float\n",
    "    max_steps: float\n",
    "\n",
    "    def __call__(self, it: int) -> float:\n",
    "        # 1) linear warmup for warmup_iters steps\n",
    "        if it < self.warmup_steps:\n",
    "            return self.max_lr * (it + 1) / self.warmup_steps\n",
    "        # 2) if it > lr_decay_iters, return min learning rate\n",
    "        if it > self.max_steps:\n",
    "            return self.min_lr\n",
    "        # 3) in between, use cosine decay down to min learning rate\n",
    "        decay_ratio = (it - self.warmup_steps) / (self.max_steps - self.warmup_steps)\n",
    "        assert 0 <= decay_ratio <= 1\n",
    "        coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # starts at 1 and goes to 0\n",
    "        return self.min_lr + coeff * (self.max_lr - self.min_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chesslm.gpt import GPT\n",
    "from torch.nn import functional as F\n",
    "\n",
    "def evaluate_model(model: GPT, dataloader: DataLoaderLite, val_steps: int = 20) -> float:\n",
    "    model.eval()\n",
    "    dataloader.reset()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        for _ in range(val_steps):\n",
    "            x, y = dataloader.next_batch()\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            with torch.autocast(device_type=DEVICE_TYPE, dtype=torch.bfloat16):\n",
    "                _, loss = model(x, y)\n",
    "            loss = loss / val_steps\n",
    "            val_loss += loss.detach().item()\n",
    "    return val_loss\n",
    "\n",
    "def write_model_checkpoint(\n",
    "        model: GPT,\n",
    "        step: int,\n",
    "        val_loss: float,\n",
    "        fpath: str\n",
    "    ):\n",
    "    checkpoint = {\n",
    "        'model': model.state_dict(),\n",
    "        'config': model.config,\n",
    "        'step': step,\n",
    "        'val_loss': val_loss\n",
    "    }\n",
    "    # you might also want to add optimizer.state_dict() and\n",
    "    # rng seeds etc., if you wanted to more exactly resume training\n",
    "    torch.save(checkpoint, fpath)\n",
    "\n",
    "def generate_sequences(model: GPT, prefix: str, num_sequences: int, max_length: int) -> list[str]:\n",
    "    tokens = ENCODER.encode(prefix)\n",
    "    tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "    tokens = tokens.unsqueeze(0).repeat(num_sequences, 1)\n",
    "    xgen = tokens.to(DEVICE)\n",
    "\n",
    "    sample_rng = torch.Generator(device=DEVICE)\n",
    "    sample_rng.manual_seed(SEED)\n",
    "\n",
    "    while xgen.size(1) < max_length:\n",
    "        # forward the model to get the logits\n",
    "        with torch.autocast(device_type=DEVICE_TYPE, dtype=torch.bfloat16):\n",
    "            logits, _ = model(xgen) # (B, T, vocab_size)\n",
    "\n",
    "        # take the logits at the last position\n",
    "        logits = logits[:, -1, :] # (B, vocab_size)\n",
    "\n",
    "        # get the probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        # do top-k sampling of 50 (huggingface pipeline default)\n",
    "        # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n",
    "        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
    "\n",
    "        # select a token from the top-k probabilities\n",
    "        # note: multinomial does not demand the input to sum to 1\n",
    "        ix = torch.multinomial(topk_probs, 1, generator=sample_rng) # (B, 1)\n",
    "\n",
    "        # gather the corresponding indices\n",
    "        xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
    "\n",
    "        # append to the sequence\n",
    "        xgen = torch.cat((xgen, xcol), dim=1)\n",
    "\n",
    "    return [ENCODER.decode(x[:max_length].tolist()) for x in xgen]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chesslm.gpt import GPTConfig\n",
    "\n",
    "MAX_STEPS = 100\n",
    "WARMUP_STEPS = 10\n",
    "\n",
    "PRINT_TRAIN_INTERVAL = 5\n",
    "EVAL_INTERVAL = 25\n",
    "EVAL_NUM_SEQUENCES = 4\n",
    "EVAL_SEQUENCE_MAX_LENGTH = 32\n",
    "\n",
    "MAX_LR = 6e-4\n",
    "MIN_LR = MAX_LR * 0.1\n",
    "WEIGHT_DECAY = 0.1\n",
    "GRAD_ACCUM_STEPS = 2\n",
    "\n",
    "T = 512\n",
    "GPT_CONFIG = GPTConfig(\n",
    "    block_size=T,\n",
    "    vocab_size=ENCODER.n_vocab,\n",
    "    n_layer=4,\n",
    "    n_head=4,\n",
    "    n_embd=384,\n",
    ")\n",
    "\n",
    "B = 4\n",
    "BT = B * T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num decayed parameter tensors: 18, with 26,573,184 parameters\n",
      "num non-decayed parameter tensors: 34, with 20,736 parameters\n",
      "using fused AdamW: False\n",
      "\n",
      "--------\n",
      "Validation loss: 10.9735\n",
      "Sample 1: POMPEY:\\n'Twas never merry world since,  clumsyãƒ cruelty attribute embryholflameenaries questions threatens fire Trin communion retard.................. threatensCVE\n",
      "Sample 2: POMPEY:\\n'Twas never merry world since, weeney taxis Hook Scalia pseausibleessorsmorphRGB450 $\\ging passing Tackle convince negotiatedCertain\n",
      "Sample 3: POMPEY:\\n'Twas never merry world since,  Pain DeskCVEUST crueltyizu GREEN coff Organ Cerweeney Proced Prep cellsoby BP pounding\n",
      "Sample 4: POMPEY:\\n'Twas never merry world since,  GREENorderingenaries HER Osirisayer KeaneKEY voice UnemploymentUST sophcand Squid Squid trial Herbert\n",
      "--------\n",
      "\n",
      "step     0 | loss: 10.974515 | lr 6.0000e-05 | norm: 6.2157 | dt: 1912.00ms | tok/sec: 2142.26\n",
      "step     5 | loss: 9.492013 | lr 3.6000e-04 | norm: 3.3044 | dt: 542.63ms | tok/sec: 7548.46\n",
      "step    10 | loss: 8.664078 | lr 6.0000e-04 | norm: 2.5692 | dt: 540.05ms | tok/sec: 7584.49\n",
      "step    15 | loss: 7.583727 | lr 5.9590e-04 | norm: 2.2203 | dt: 562.98ms | tok/sec: 7275.55\n",
      "step    20 | loss: 7.038090 | lr 5.8372e-04 | norm: 1.5015 | dt: 531.24ms | tok/sec: 7710.30\n",
      "\n",
      "--------\n",
      "Validation loss: 6.7080\n",
      "Sample 1: POMPEY:\\n'Twas never merry world since, 'd\\n what.\\n I,\\n\\n.\\n this and he my'.\n",
      "Sample 2: POMPEY:\\n'Twas never merry world since,  my:\\n of good,,\\n\\n\\n.\\n\\n\\n my of the\n",
      "Sample 3: POMPEY:\\n'Twas never merry world since, ,\\n\\n:\\n and it of; the a-,And it! I\n",
      "Sample 4: POMPEY:\\n'Twas never merry world since,  the,,\\n, the,\\n\\n.\\nAnd\\n in it!\\n\n",
      "--------\n",
      "\n",
      "step    25 | loss: 6.738054 | lr 5.6383e-04 | norm: 0.8152 | dt: 1882.75ms | tok/sec: 2175.54\n",
      "step    30 | loss: 6.421026 | lr 5.3683e-04 | norm: 0.9769 | dt: 538.76ms | tok/sec: 7602.68\n",
      "step    35 | loss: 6.490775 | lr 5.0355e-04 | norm: 1.1506 | dt: 532.44ms | tok/sec: 7692.94\n",
      "step    40 | loss: 6.238823 | lr 4.6500e-04 | norm: 0.7919 | dt: 528.83ms | tok/sec: 7745.40\n",
      "step    45 | loss: 6.196313 | lr 4.2235e-04 | norm: 0.7576 | dt: 533.04ms | tok/sec: 7684.21\n",
      "\n",
      "--------\n",
      "Validation loss: 6.4204\n",
      "Sample 1: POMPEY:\\n'Twas never merry world since, !\\nWho you to that thou have and I thee d I her?\\nI\n",
      "Sample 2: POMPEY:\\n'Twas never merry world since,  and I have;\\n\\nAnd,\\n\\nIBut will be that to and\n",
      "Sample 3: POMPEY:\\n'Twas never merry world since, ,\\nThat my father withH.\\nTo I your this you thy in you\n",
      "Sample 4: POMPEY:\\n'Twas never merry world since, ;\\nI my myL theAnd the the me that, be have for,\n",
      "--------\n",
      "\n",
      "step    50 | loss: 6.146143 | lr 3.7689e-04 | norm: 0.8133 | dt: 1873.50ms | tok/sec: 2186.28\n",
      "step    55 | loss: 6.282887 | lr 3.3000e-04 | norm: 0.7522 | dt: 530.53ms | tok/sec: 7720.62\n",
      "step    60 | loss: 6.221033 | lr 2.8311e-04 | norm: 0.7499 | dt: 556.49ms | tok/sec: 7360.42\n",
      "step    65 | loss: 6.191391 | lr 2.3765e-04 | norm: 0.9613 | dt: 528.48ms | tok/sec: 7750.50\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "no tokens remaining",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 51\u001b[0m\n\u001b[1;32m     49\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m micro_step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(GRAD_ACCUM_STEPS):\n\u001b[0;32m---> 51\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(DEVICE), y\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautocast(device_type\u001b[38;5;241m=\u001b[39mDEVICE_TYPE, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16):\n",
      "Cell \u001b[0;32mIn[9], line 49\u001b[0m, in \u001b[0;36mDataLoaderLite.next_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnext_batch\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m---> 49\u001b[0m     batch_tokens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43madd_pred_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         x \u001b[38;5;241m=\u001b[39m (batch_tokens[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mB, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mT) \u001b[38;5;66;03m# inputs\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 37\u001b[0m, in \u001b[0;36mDataLoaderLite.next_tokens\u001b[0;34m(self, add_pred_token)\u001b[0m\n\u001b[1;32m     35\u001b[0m segment \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pos : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pos \u001b[38;5;241m+\u001b[39m pos_step]\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m(segment):\n\u001b[0;32m---> 37\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno tokens remaining\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m match \u001b[38;5;241m:=\u001b[39m re\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mW+\u001b[39m\u001b[38;5;124m\"\u001b[39m, segment[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]):\n\u001b[1;32m     39\u001b[0m     segment \u001b[38;5;241m=\u001b[39m segment[: \u001b[38;5;28mlen\u001b[39m(segment) \u001b[38;5;241m-\u001b[39m match\u001b[38;5;241m.\u001b[39mend() \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: no tokens remaining"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "model = GPT(GPT_CONFIG).to(DEVICE)\n",
    "\n",
    "cosine_schedule = CosineLRSchedule(\n",
    "    max_lr=MAX_LR,\n",
    "    min_lr=MIN_LR,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    max_steps=MAX_STEPS,\n",
    ")\n",
    "\n",
    "train_loader = DataLoaderLite(B=B, T=T, text=train_text)\n",
    "val_loader = DataLoaderLite(B=B, T=T, text=val_text, loop=True)\n",
    "\n",
    "optimizer = model.configure_optimizers(\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    learning_rate=MAX_LR,\n",
    "    device_type=DEVICE_TYPE\n",
    ")\n",
    "\n",
    "for step in range(MAX_STEPS):\n",
    "    t0 = time.time()\n",
    "    last_step = (step == MAX_STEPS - 1)\n",
    "\n",
    "    # once in a while:\n",
    "    # evaluate our validation loss\n",
    "    # generate from the model and print\n",
    "    if step % EVAL_INTERVAL == 0 or last_step:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = evaluate_model(model, val_loader)\n",
    "            sequences = generate_sequences(\n",
    "                model,\n",
    "                \"POMPEY:\\n'Twas never merry world since, \",\n",
    "                EVAL_NUM_SEQUENCES,\n",
    "                EVAL_SEQUENCE_MAX_LENGTH,\n",
    "            )\n",
    "\n",
    "        print(\"\\n--------\")\n",
    "        print(f\"Validation loss: {val_loss:.4f}\")\n",
    "        for i, seq in enumerate(sequences, 1):\n",
    "            seq = seq.replace('\\n', '\\\\n')\n",
    "            print(f\"Sample {i}: {seq}\")\n",
    "        print(\"--------\\n\")\n",
    "\n",
    "    # do one step of the optimization\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    train_loss = 0.0\n",
    "    for micro_step in range(GRAD_ACCUM_STEPS):\n",
    "        x, y = train_loader.next_batch()\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "\n",
    "        with torch.autocast(device_type=DEVICE_TYPE, dtype=torch.bfloat16):\n",
    "            logits, loss = model(x, y)\n",
    "\n",
    "        loss = loss / GRAD_ACCUM_STEPS\n",
    "        loss.backward()\n",
    "\n",
    "        train_loss += loss.detach().item()\n",
    "\n",
    "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr = cosine_schedule(step)\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    if DEVICE_TYPE == \"cuda\":\n",
    "        torch.cuda.synchronize() # wait for the GPU to finish work\n",
    "\n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0 # time difference in seconds\n",
    "    tokens_processed = BT * GRAD_ACCUM_STEPS\n",
    "    tokens_per_sec = tokens_processed / dt\n",
    "\n",
    "    if step % PRINT_TRAIN_INTERVAL == 0:\n",
    "        print(\n",
    "            f\"step {step:5d}\"\n",
    "            f\" | loss: {train_loss:.6f}\"\n",
    "            f\" | lr {lr:.4e}\"\n",
    "            f\" | norm: {norm:.4f}\"\n",
    "            f\" | dt: {dt*1000:.2f}ms\"\n",
    "            f\" | tok/sec: {tokens_per_sec:.2f}\"\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
