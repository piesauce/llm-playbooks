{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning a Gemma model for Q & A\n",
    "\n",
    "https://medium.com/the-ai-forum/instruction-fine-tuning-gemma-2b-on-medical-reasoning-and-convert-the-finetuned-model-into-gguf-844191f8d329"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_LIMIT = 1024\n",
    "TRAIN_SIZE = 1000\n",
    "TEST_SIZE = 100\n",
    "EVAL_SIZE = 50\n",
    "SEED = 123\n",
    "\n",
    "BASE_MODEL_ID = \"google/gemma-2b-it\"\n",
    "# BASE_MODEL_ID = \"google/gemma-2-9b-it\"\n",
    "GENERATE_KWARGS = dict(\n",
    "    do_sample=True,\n",
    "    max_new_tokens=512,\n",
    "    temperature=1e-3,\n",
    ")\n",
    "EVAL_BATCH_SIZE = 4\n",
    "\n",
    "TRAIN_MAX_LENGTH = 512\n",
    "TRAIN_NUM_EPOCHS = 1\n",
    "TRAIN_BATCH_SIZE = 4\n",
    "TRAIN_GRADIENT_ACCUMULATION_STEPS = 1\n",
    "TRAIN_LOGGING_STEPS = 10\n",
    "EVAL_ACCUMULATION_STEPS = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Gemma 2B instruct model\n",
    "\n",
    "https://huggingface.co/google/gemma-2b-it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4f3391ba9c846cbbd9a27d4dbbae2fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID)\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_storage=torch.bfloat16,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_ID,\n",
    "    quantization_config=quantization_config\n",
    ")\n",
    "print(model.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Medical Q & A\n",
    "\n",
    "https://huggingface.co/datasets/medalpaca/medical_meadow_medqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input', 'output'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input', 'output'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# input\n",
      "Q:A 45-year-old man comes to the physician because of worsening shortness of breath and dry cough for 6 months. The patient's symptoms get worse when he walks more than about 150 yards. He also reports fatigue and difficulty swallowing solid foods. In cold weather, his fingers occasionally turn blue and become painful. He occasionally smokes cigarettes on weekends. His temperature is 37°C (98.6°F), and respirations are 22/min, pulse is 87/min, and blood pressure is 126/85 mm Hg. The skin over his trunk and arms is thickened and tightened. Fine inspiratory crackles are heard over bilateral lower lung fields on auscultation. Which of the following additional findings is most likely in this patient?? \n",
      "{'A': 'Decreased right atrial pressure', 'B': 'Increased lung compliance', 'C': 'Decreased diffusing capacity', 'D': 'Increased airway resistance', 'E': 'Decreased A-a gradient'},\n",
      "\n",
      "# output\n",
      "C: Decreased diffusing capacity\n"
     ]
    }
   ],
   "source": [
    "from medqa_data import load_train_test_data\n",
    "\n",
    "dataset = load_train_test_data(\n",
    "    train_size=TRAIN_SIZE,\n",
    "    test_size=TEST_SIZE,\n",
    "    seed=SEED,\n",
    "    input_limit=INPUT_LIMIT,\n",
    ")\n",
    "display(dataset)\n",
    "\n",
    "def print_sample(sample: dict[str, str]):\n",
    "    print(\"\\n\".join(f\"\\n# {k}\\n{v}\" for k, v in sample.items())[1:])\n",
    "\n",
    "print_sample(dataset[\"test\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "Q:A child is in the nursery one day after birth. A nurse notices a urine-like discharge being expressed through the umbilical stump. What two structures in the embryo are connected by the structure that failed to obliterate during the embryologic development of this child??\n",
      "{'A': 'Pulmonary artery - aorta', 'B': 'Bladder - yolk sac', 'C': 'Bladder - small bowel', 'D': 'Liver - umbilical vein', 'E': 'Kidney - large bowel'},\n",
      "Give your answer as a JSON dictionary with the \"option\" (a letter from A-E) and the  corresponding\"option_text\". No yapping.<end_of_turn>\n",
      "<start_of_turn>model\n",
      "The correct answer is **B**: The bladder and the yolk sac.\n",
      "\n",
      "The bladder and the yolk sac are connected by the structure that failed to obliterate during the embryologic development of this child.<eos>\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\"\n",
    "Q:A child is in the nursery one day after birth. A nurse notices a urine-like discharge being expressed through the umbilical stump. What two structures in the embryo are connected by the structure that failed to obliterate during the embryologic development of this child??\n",
    "{'A': 'Pulmonary artery - aorta', 'B': 'Bladder - yolk sac', 'C': 'Bladder - small bowel', 'D': 'Liver - umbilical vein', 'E': 'Kidney - large bowel'},\n",
    "Give your answer as a JSON dictionary with the \"option\" (a letter from A-E) and the  corresponding\"option_text\". No yapping.\n",
    "\"\"\".strip()\n",
    "\n",
    "chat = [{\"role\": \"user\", \"content\": question}]\n",
    "input_ids = tokenizer.apply_chat_template(chat, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n",
    "output_ids = model.generate(input_ids, **GENERATE_KWARGS)\n",
    "input_ids = input_ids.to(\"cpu\")\n",
    "output_ids = output_ids.to(\"cpu\")\n",
    "print(tokenizer.decode(output_ids[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# input\n",
      "Q: A 45-year-old man comes to the physician because of worsening shortness of breath and dry cough for 6 months. The patient's symptoms get worse when he walks more than about 150 yards. He also reports fatigue and difficulty swallowing solid foods. In cold weather, his fingers occasionally turn blue and become painful. He occasionally smokes cigarettes on weekends. His temperature is 37°C (98.6°F), and respirations are 22/min, pulse is 87/min, and blood pressure is 126/85 mm Hg. The skin over his trunk and arms is thickened and tightened. Fine inspiratory crackles are heard over bilateral lower lung fields on auscultation. Which of the following additional findings is most likely in this patient?? \n",
      "{'A': 'Decreased right atrial pressure', 'B': 'Increased lung compliance', 'C': 'Decreased diffusing capacity', 'D': 'Increased airway resistance', 'E': 'Decreased A-a gradient'}\n",
      "Give your answer as a JSON dictionary with the \"option\" (a letter from A-E) and the  corresponding\"option_text\". No yapping.\n",
      "\n",
      "# output\n",
      "{\"option\": \"C\", \"text\": \"Decreased diffusing capacity\"}\n",
      "\n",
      "# true_label\n",
      "C\n"
     ]
    }
   ],
   "source": [
    "from medqa_data import reformat_sample\n",
    "\n",
    "dataset = dataset.map(reformat_sample)\n",
    "train_data, test_data = dataset[\"train\"], dataset[\"test\"]\n",
    "\n",
    "print_sample(test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function create_predict.<locals>._batch_predict at 0x7f3cce138040> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1bd97a736e9436c8630ac0c9100f570",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemma accuracy: 19.0%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from medqa_data import create_predict\n",
    "\n",
    "batch_predict = create_predict(tokenizer, model, \"gemma\", batch=True, generate_kwargs=GENERATE_KWARGS)\n",
    "test_data = test_data.map(batch_predict, batched=True, batch_size=EVAL_BATCH_SIZE)\n",
    "\n",
    "gemma_accuracy = (np.asarray(test_data[\"gemma_label\"]) == np.asarray(test_data[\"true_label\"])).mean()\n",
    "print(f\"Gemma accuracy: {round(gemma_accuracy * 100, 1)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised finetuning (SFT)\n",
    "\n",
    "https://huggingface.co/docs/trl/en/sft_trainer\n",
    "https://huggingface.co/docs/bitsandbytes/main/en/fsdp_qlora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start_of_turn>user\\nQ: A 63-year-old man with a history of hypertension and atrial fibrillation is brought into the emergency room and found to have a ventricular tachyarrhythmia. Ibutilide is discontinued and the patient is switched to another drug that also prolongs the QT interval but is associated with a decreased risk of torsades de pointes. Which drug was most likely administered in this patient?? \\n{\\'A\\': \\'Sotalol\\', \\'B\\': \\'Digoxin\\', \\'C\\': \\'Esmolol\\', \\'D\\': \\'Amiodarone\\', \\'E\\': \\'Quinidine\\'}\\nGive your answer as a JSON dictionary with the \"option\" (a letter from A-E) and the  corresponding\"option_text\". No yapping.<end_of_turn>\\n<start_of_turn>model\\n{\"option\": \"D\", \"text\": \"Amiodarone\"}<end_of_turn>\\n',\n",
       " '<start_of_turn>user\\nQ: An investigator is studying a drug that acts on the thyroid hormone pathway. Levels of serum free T3 and T4 in healthy participants are measured before and after administration of the drug. After administration, there is a decrease in the average serum free T3 level, while the average serum free T4 level is increased compared to initial serum studies. Inhibition of which of the following is the most likely mechanism of action of this drug?? \\n{\\'A\\': \\'Follicular thyroid proteases\\', \\'B\\': \\'Thyroid-stimulating hormone\\', \\'C\\': \\'Follicular iodotyrosine deiodinase\\', \\'D\\': \\'Follicular thyroid peroxidase\\', \\'E\\': \"Peripheral 5\\'-deiodinase\"}\\nGive your answer as a JSON dictionary with the \"option\" (a letter from A-E) and the  corresponding\"option_text\". No yapping.<end_of_turn>\\n<start_of_turn>model\\n{\"option\": \"E\", \"text\": \"Peripheral 5\\'-deiodinase\"}<end_of_turn>\\n',\n",
       " '<start_of_turn>user\\nQ: A 25-year-old man is admitted to the hospital after a severe motor vehicle accident as an unrestrained front-seat passenger. Appropriate life-saving measures are given, and the patient is now hemodynamically stable. Physical examination shows a complete loss of consciousness. There are no motor or ocular movements with painful stimuli. The patient has bilaterally intact pupillary light reflexes. The patient is placed in a 30° semi-recumbent position for further examination. What is the most likely finding on the examination of this patient\\'s right ear?? \\n{\\'A\\': \\'Cold water causing ipsilateral saccadic movement.\\', \\'B\\': \\'Warm water causing ipsilateral slow pursuit.\\', \\'C\\': \\'Warm water causing ipsilateral saccadic movement.\\', \\'D\\': \\'Warm water mimicking the head turning left.\\', \\'E\\': \\'Cold water causing contralateral slow pursuit.\\'}\\nGive your answer as a JSON dictionary with the \"option\" (a letter from A-E) and the  corresponding\"option_text\". No yapping.<end_of_turn>\\n<start_of_turn>model\\n{\"option\": \"C\", \"text\": \"Warm water causing ipsilateral saccadic movement.\"}<end_of_turn>\\n']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_chat_for_finetuning(samples: dict) -> list[str]:\n",
    "    chat_texts = tokenizer.apply_chat_template(\n",
    "        [\n",
    "            [\n",
    "                {\"role\": \"user\", \"content\": input},\n",
    "                {\"role\": \"assistant\", \"content\": output},\n",
    "            ]\n",
    "            for input, output in zip(samples[\"input\"], samples[\"output\"])\n",
    "        ],\n",
    "        tokenize=False,\n",
    "    )\n",
    "    return [text.removeprefix(\"<bos>\") for text in chat_texts]\n",
    "\n",
    "display(create_chat_for_finetuning(train_data[:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable: 78,446,592 | total: 2,584,619,008 | Percentage: 3.0351%\n"
     ]
    }
   ],
   "source": [
    "# pip install peft\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=\"all-linear\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "trainable, total = model.get_nb_trainable_parameters()\n",
    "print(f\"Trainable: {trainable:,} | total: {total:,} | Percentage: {trainable/total*100:.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37965e13a87d41978372e65172232a4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 08:46, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.727800</td>\n",
       "      <td>1.500449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.258600</td>\n",
       "      <td>0.914189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.781900</td>\n",
       "      <td>0.592388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.542400</td>\n",
       "      <td>0.391848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.311700</td>\n",
       "      <td>0.260579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.214000</td>\n",
       "      <td>0.204531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.216300</td>\n",
       "      <td>0.163737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.139700</td>\n",
       "      <td>0.149012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.112500</td>\n",
       "      <td>0.142390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.157000</td>\n",
       "      <td>0.138367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.162400</td>\n",
       "      <td>0.130917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.176000</td>\n",
       "      <td>0.129024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.118000</td>\n",
       "      <td>0.127040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.113000</td>\n",
       "      <td>0.125021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.108200</td>\n",
       "      <td>0.123154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.110900</td>\n",
       "      <td>0.120749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.137500</td>\n",
       "      <td>0.118864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.100300</td>\n",
       "      <td>0.117453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.105100</td>\n",
       "      <td>0.117422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.125400</td>\n",
       "      <td>0.117017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.135600</td>\n",
       "      <td>0.116530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.113000</td>\n",
       "      <td>0.116029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.125800</td>\n",
       "      <td>0.115587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.115100</td>\n",
       "      <td>0.115443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.106400</td>\n",
       "      <td>0.115411</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'global_step': 250,\n",
       " 'training_loss': 0.33258164548873903,\n",
       " 'metrics': {'train_runtime': 527.8125,\n",
       "  'train_samples_per_second': 1.895,\n",
       "  'train_steps_per_second': 0.474,\n",
       "  'total_flos': 3622111360008192.0,\n",
       "  'train_loss': 0.33258164548873903,\n",
       "  'epoch': 1.0}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import DataCollatorForCompletionOnlyLM, SFTConfig, SFTTrainer\n",
    "\n",
    "tokenizer.padding_side = \"right\"\n",
    "model.config.use_cache=False\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "collator = DataCollatorForCompletionOnlyLM(\n",
    "    instruction_template=\"<start_of_turn>user\\n\",\n",
    "    response_template=\"<start_of_turn>model\\n\",\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model,\n",
    "    args=SFTConfig(\n",
    "        output_dir=\"/tmp/finetuned_gemma_2b\",\n",
    "        per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
    "        gradient_accumulation_steps=TRAIN_GRADIENT_ACCUMULATION_STEPS,\n",
    "        gradient_checkpointing=True,\n",
    "        gradient_checkpointing_kwargs=dict(use_reentrant=False),\n",
    "        max_seq_length=TRAIN_MAX_LENGTH,\n",
    "        num_train_epochs=TRAIN_NUM_EPOCHS,\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_steps=TRAIN_LOGGING_STEPS,\n",
    "        eval_steps=TRAIN_LOGGING_STEPS,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_accumulation_steps=EVAL_ACCUMULATION_STEPS,\n",
    "    ),\n",
    "    data_collator=collator,\n",
    "    eval_dataset=test_data.select(range(min(len(test_data), EVAL_SIZE))),\n",
    "    formatting_func=create_chat_for_finetuning,\n",
    "    peft_config=lora_config,\n",
    "    train_dataset=train_data,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "train_result = trainer.train()\n",
    "display(train_result._asdict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.use_cache=True\n",
    "model.gradient_checkpointing_disable()\n",
    "model.eval()\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge and evaluate the finetuned model\n",
    "\n",
    "After training the LoRA adapter has not yet been merged with the base Gemma model. This will make it run a lot slower. To merge the LoRA adapter, we will follow these steps:\n",
    "- https://huggingface.co/docs/peft/main/en/package_reference/lora#peft.LoraModel.merge_and_unload\n",
    "- https://discuss.huggingface.co/t/help-with-merging-lora-weights-back-into-base-model/40968/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d686843a3253492e8e8c29c79e96eed5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "trainer.model.save_pretrained(\"models/lora_adapter\")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_ID,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, \"models/lora_adapter\").merge_and_unload()\n",
    "model.save_pretrained(\"models/finetuned_model\", safe_serialization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's clean up all our prior models from GPU memory, and load the merged model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4d3f70b5665465ba1662a4a9ca71309",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "try: del model\n",
    "except NameError: pass\n",
    "try: del trainer\n",
    "except NameError: pass\n",
    "try: del base_model\n",
    "except NameError: pass\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"models/finetuned_model\",\n",
    "    quantization_config=quantization_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dad00a634de14a23820f902b65ccf8c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemma accuracy: 19.0%\n",
      "Finetuned accuracy: 25.0%\n"
     ]
    }
   ],
   "source": [
    "batch_predict = create_predict(tokenizer, model, \"finetuned\", batch=True, generate_kwargs=GENERATE_KWARGS)\n",
    "test_data = test_data.map(batch_predict, batched=True, batch_size=EVAL_BATCH_SIZE)\n",
    "\n",
    "finetuned_accuracy = (np.asarray(test_data[\"finetuned_label\"]) == np.asarray(test_data[\"true_label\"])).mean()\n",
    "print(f\"Gemma accuracy: {round(gemma_accuracy * 100, 1)}%\")\n",
    "print(f\"Finetuned accuracy: {round(finetuned_accuracy * 100, 1)}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
