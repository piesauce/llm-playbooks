{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building LangChain Agents with Multiple LLM Providers along with Tracing from Langsmith\n",
    "\n",
    "This notebook demonstrates how to create and use LangChain agents with different LLM providers (OpenAI and Together AI). You'll learn how agents use tools to answer queries by selecting the appropriate resources based on the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary dependencies\n",
    "!pip install langchain langchain-openai langchain-together langchain-experimental langchain-community langsmith\n",
    "!pip install wikipedia duckduckgo-search arxiv yfinance python-dotenv\n",
    "\n",
    "# Note: You may need to restart the kernel after installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup and Dependencies\n",
    "Import all necessary libraries including LangChain components, OpenAI integration, and Together AI integration. Explain what each import is used for in the context of building an agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os  # For setting environment variables\n",
    "\n",
    "# Imnport LangSmith components\n",
    "from langsmith import traceable # For tracing function calls\n",
    "\n",
    "# Import LangChain components\n",
    "from langchain.agents import initialize_agent, Tool  # For initializing the agent and defining tools\n",
    "from langchain_openai import ChatOpenAI  # For OpenAI integration\n",
    "from langchain_together import ChatTogether  # For Together AI integration\n",
    "from langchain_experimental.utilities import PythonREPL  # For executing Python code\n",
    "from langchain_community.utilities import WikipediaAPIWrapper  # For Wikipedia API wrapper\n",
    "from langchain_community.tools.yahoo_finance_news import YahooFinanceNewsTool  # For Yahoo Finance News tool\n",
    "from langchain_community.tools import (\n",
    "    WikipediaQueryRun,  # For running Wikipedia queries\n",
    "    DuckDuckGoSearchResults,  # For running DuckDuckGo search queries\n",
    "    ArxivQueryRun,  # For running arXiv queries\n",
    ")\n",
    "\n",
    "# Set the USER_AGENT environment variable to avoid warnings\n",
    "os.environ[\"USER_AGENT\"] = \"Chapter_10_Agent/1.0\"\n",
    "\n",
    "# Set the Langsmith env variables\n",
    "\n",
    "os.environ[\"LANGSMITH_TRACE\"] = \"True\"  # Enable tracing\n",
    "os.environ[\"LANGSMITH_ENDPOINT\"] = \"https://api.smith.langchain.com\"  # Set the LangSmith endpoint\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = \"\"  # Enter Your LangSmith API Key\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"chapter-10\"  # Set the LangSmith project\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure API Keys\n",
    "Set up environment variables for both OpenAI and Together AI API keys. Include code to check if keys are properly configured and instructions on how to obtain these keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure API Keys\n",
    "\n",
    "# Set up environment variables for OpenAI and Together AI API keys\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "os.environ[\"TOGETHER_API_KEY\"] = \"\"\n",
    "\n",
    "# Check if the API keys are properly configured\n",
    "assert \"OPENAI_API_KEY\" in os.environ, \"OpenAI API key is not set. Please set the OPENAI_API_KEY environment variable.\"\n",
    "assert \"TOGETHER_API_KEY\" in os.environ, \"Together AI API key is not set. Please set the TOGETHER_API_KEY environment variable.\"\n",
    "\n",
    "# Instructions on how to obtain these keys\n",
    "# - OpenAI API Key: Sign up at https://beta.openai.com/signup/ and generate an API key from the API section.\n",
    "# - Together AI API Key: Sign up at https://together.ai/ and generate an API key from the API section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Available Tools\n",
    "Create and configure the tools that will be available to the agent: Wikipedia, DuckDuckGo search, ArXiv query, Python REPL, and Yahoo Finance News. Include explanations of what each tool does and when it should be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Wikipedia API wrapper and create the tool\n",
    "wiki_api = WikipediaAPIWrapper()\n",
    "wiki_tool = WikipediaQueryRun(api_wrapper=wiki_api)\n",
    "\n",
    "# Define the tools with clear descriptions\n",
    "tools = [\n",
    "    wiki_tool,\n",
    "    DuckDuckGoSearchResults(),\n",
    "    ArxivQueryRun(),\n",
    "    Tool(\n",
    "        name=\"Python REPL\",\n",
    "        func=PythonREPL().run,\n",
    "        description=\"A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\",\n",
    "    ),\n",
    "    YahooFinanceNewsTool(),\n",
    "]\n",
    "\n",
    "# Explanation:\n",
    "# - WikipediaQueryRun: Uses the Wikipedia API to fetch information from Wikipedia.\n",
    "# - DuckDuckGoSearchResults: Uses DuckDuckGo to fetch search results.\n",
    "# - ArxivQueryRun: Uses arXiv to fetch scientific research papers.\n",
    "# - Python REPL: Executes Python code snippets.\n",
    "# - YahooFinanceNewsTool: Fetches financial news from Yahoo Finance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model Provider Function\n",
    "Implement a function that creates an LLM based on the provider choice (OpenAI or Together AI). Include parameters for model selection, temperature, and other relevant settings for each provider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Model Provider Function\n",
    "def create_llm(provider: str, model: str, temperature: float = 0, max_tokens: int = None, timeout: int = None, max_retries: int = 2):\n",
    "    \"\"\"\n",
    "    Create an LLM based on the provider choice (OpenAI or Together AI).\n",
    "    \n",
    "    Parameters:\n",
    "    - provider (str): The LLM provider ('openai' or 'together').\n",
    "    - model (str): The model name to use.\n",
    "    - temperature (float): The temperature setting for the model.\n",
    "    - max_tokens (int): The maximum number of tokens for the response.\n",
    "    - timeout (int): The timeout setting for the model.\n",
    "    - max_retries (int): The maximum number of retries for the model.\n",
    "    \n",
    "    Returns:\n",
    "    - llm: The initialized LLM.\n",
    "    \"\"\"\n",
    "    if provider == \"openai\":\n",
    "        return ChatOpenAI(\n",
    "            model=model,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "            timeout=timeout,\n",
    "            max_retries=max_retries,\n",
    "        )\n",
    "    elif provider == \"together\":\n",
    "        return ChatTogether(\n",
    "            model=model,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "            timeout=timeout,\n",
    "            max_retries=max_retries,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported provider. Choose either 'openai' or 'together'.\")\n",
    "\n",
    "# Example usage:\n",
    "# llm_openai = create_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n",
    "# llm_together = create_llm(provider=\"together\", model=\"together-gpt-neoxt-chat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define System Message\n",
    "Create the system message that instructs the agent on how to select the appropriate tool based on the query type. Explain how the system message influences agent behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the system message that instructs the agent on how to select the appropriate tool based on the query type\n",
    "system_message = (\n",
    "    \"You are Charlie, a helpful intelligent agent that selects the most appropriate tool for each query and provides the best response. \"\n",
    "    \"If the query asks for financial analysis (especially stock-related news), use the Yahoo Finance News tool. \"\n",
    "    \"For general information, use Wikipedia or DuckDuckGo. \"\n",
    "    \"If the query is about scientific papers or research (for example, papers on LLMs), use arXiv. \"\n",
    "    \"For executing code or calculations, use the PythonREPL tool. \"\n",
    "    \"Always decide based on the context of the user's request.\"\n",
    ")\n",
    "\n",
    "# Explanation:\n",
    "# - The system message guides the agent on which tool to use based on the type of query.\n",
    "# - It ensures that the agent selects the most appropriate tool for financial analysis, general information, scientific research, or code execution.\n",
    "# - This message influences the agent's behavior by providing clear instructions on tool selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Initialization\n",
    "Create a function that initializes the agent with the chosen LLM provider and tools. Include explanation of how the zero-shot-react-description agent works and its decision-making process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Agent Initialization Function\n",
    "def initialize_agent_with_provider(provider: str, model: str):\n",
    "    \"\"\"\n",
    "    Initialize the agent with the chosen LLM provider and tools.\n",
    "    \n",
    "    Parameters:\n",
    "    - provider (str): The LLM provider ('openai' or 'together').\n",
    "    - model (str): The model name to use.\n",
    "    \n",
    "    Returns:\n",
    "    - agent: The initialized agent.\n",
    "    \"\"\"\n",
    "    # Create the LLM based on the provider\n",
    "    llm = create_llm(provider=provider, model=model)\n",
    "    \n",
    "    # Initialize the agent with the LLM and tools\n",
    "    agent = initialize_agent(\n",
    "        tools,\n",
    "        llm,\n",
    "        agent=\"zero-shot-react-description\",\n",
    "        verbose=True,\n",
    "        agent_kwargs={\"system_message\": system_message},\n",
    "    )\n",
    "    \n",
    "    return agent\n",
    "\n",
    "# Explanation:\n",
    "# - This function initializes the agent with the chosen LLM provider (OpenAI or Together AI) and the predefined tools.\n",
    "# - It uses the `create_llm` function to create the LLM based on the provider and model.\n",
    "# - The `initialize_agent` function sets up the agent with the LLM, tools, and system message.\n",
    "# - The agent is configured to use the \"zero-shot-react-description\" agent type, which selects the appropriate tool based on the query.\n",
    "# - The `verbose` parameter is set to True to provide detailed output during the agent's operation.\n",
    "\n",
    "# Example usage:\n",
    "# agent_openai = initialize_agent_with_provider(provider=\"openai\", model=\"gpt-4o-mini\")\n",
    "# agent_together = initialize_agent_with_provider(provider=\"together\", model=\"together-gpt-neoxt-chat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Invoke function with Trace\n",
    "\n",
    "Convert the agent invoke method into a function to apply tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the agent call with @traceable so the entire chain is logged.\n",
    "@traceable\n",
    "def run_agent(agent, query: str):\n",
    "    return agent.invoke(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Financial News Query Test\n",
    "Run a financial news query with both providers and display results. Include visualization of the agent's thought process and explanation of why it selected particular tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Financial News Query Test\n",
    "\n",
    "# Initialize agents for both OpenAI and Together AI\n",
    "agent_openai = initialize_agent_with_provider(provider=\"openai\", model=\"gpt-4o-mini\")\n",
    "agent_together = initialize_agent_with_provider(provider=\"together\", model=\"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\")\n",
    "\n",
    "## Add your custom queries here:\n",
    "\n",
    "# Run a financial news query with OpenAI \n",
    "print(\"=== Financial News Query with OpenAI ===\")\n",
    "result_openai = run_agent(agent_openai , \"What happened today with Microsoft stocks?\")\n",
    "print(\"Final Answer (OpenAI):\", result_openai)\n",
    "\n",
    "# Run a financial news query with Together AI\n",
    "print(\"\\n=== Financial News Query with Together AI ===\")\n",
    "result_together = run_agent(agent_together, \"What happened today with Microsoft stocks?\")\n",
    "print(\"Final Answer (Together AI):\", result_together)\n",
    "\n",
    "# Explanation:\n",
    "# - This section initializes agents for both OpenAI and Together AI using the `initialize_agent_with_provider` function.\n",
    "# - It runs a financial news query (\"What happened today with Microsoft stocks?\") with both agents.\n",
    "# - The results from both agents are printed for comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scientific Research Query Test\n",
    "Run a scientific research query with both providers and display results. Analyze how the agent determines which tool to use for research queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scientific Research Query Test\n",
    "\n",
    "# Initialize agents for both OpenAI and Together AI\n",
    "agent_openai = initialize_agent_with_provider(provider=\"openai\", model=\"gpt-4o-mini\")\n",
    "agent_together = initialize_agent_with_provider(provider=\"together\", model=\"together-gpt-neoxt-chat\")\n",
    "\n",
    "# Run a scientific research query with OpenAI\n",
    "print(\"\\n=== Scientific Research Query with OpenAI ===\")\n",
    "result_openai = run_agent(agent_openai, \"Find recent research papers on large language models.\")\n",
    "print(\"Final Answer (OpenAI):\", result_openai)\n",
    "\n",
    "# Run a scientific research query with Together AI\n",
    "print(\"\\n=== Scientific Research Query with Together AI ===\")\n",
    "result_together = run_agent(agent_together, \"Find recent research papers on large language models.\")\n",
    "print(\"Final Answer (Together AI):\", result_together)\n",
    "\n",
    "# Explanation:\n",
    "# - This section initializes agents for both OpenAI and Together AI using the `initialize_agent_with_provider` function.\n",
    "# - It runs a scientific research query (\"Find recent research papers on large language models.\") with both agents.\n",
    "# - The results from both agents are printed for comparison.\n",
    "# - This demonstrates how the agent determines which tool to use for research queries based on the system message instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Execution Query Test\n",
    "Run a code execution query with both providers and display results. Explain how the agent recognizes computational tasks and safely executes Python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Execution Query Test\n",
    "\n",
    "# Initialize agents for both OpenAI and Together AI\n",
    "agent_openai = initialize_agent_with_provider(provider=\"openai\", model=\"gpt-4o-mini\")\n",
    "agent_together = initialize_agent_with_provider(provider=\"together\", model=\"together-gpt-neoxt-chat\")\n",
    "\n",
    "# Run a code execution query with OpenAI\n",
    "print(\"\\n=== Code Execution Query with OpenAI ===\")\n",
    "result_openai = run_agent(agent_openai, \"Run a quick Python snippet to compute 2+2.\")\n",
    "print(\"Final Answer (OpenAI):\", result_openai)\n",
    "\n",
    "# Run a code execution query with Together AI\n",
    "print(\"\\n=== Code Execution Query with Together AI ===\")\n",
    "result_together = run_agent(agent_together, \"Run a quick Python snippet to compute 2+2.\")\n",
    "print(\"Final Answer (Together AI):\", result_together)\n",
    "\n",
    "# Explanation:\n",
    "# - This section initializes agents for both OpenAI and Together AI using the `initialize_agent_with_provider` function.\n",
    "# - It runs a code execution query (\"Run a quick Python snippet to compute 2+2.\") with both agents.\n",
    "# - The results from both agents are printed for comparison.\n",
    "# - This demonstrates how the agent recognizes computational tasks and safely executes Python code using the Python REPL tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Provider Comparison\n",
    "Compare the results from OpenAI and Together AI for the same queries. Analyze differences in reasoning patterns, tool selection behavior, and response quality between providers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provider Comparison\n",
    "\n",
    "# Initialize agents for both OpenAI and Together AI\n",
    "agent_openai = initialize_agent_with_provider(provider=\"openai\", model=\"gpt-4o-mini\")\n",
    "agent_together = initialize_agent_with_provider(provider=\"together\", model=\"together-gpt-neoxt-chat\")\n",
    "\n",
    "# Define queries to test\n",
    "queries = [\n",
    "    \"What happened today with Microsoft stocks?\",\n",
    "    \"Find recent research papers on large language models.\",\n",
    "    \"Run a quick Python snippet to compute 2+2.\"\n",
    "]\n",
    "\n",
    "# Function to run queries and compare results\n",
    "def compare_providers(queries):\n",
    "    for query in queries:\n",
    "        print(f\"\\n=== Query: {query} ===\")\n",
    "        \n",
    "        # Run query with OpenAI\n",
    "        print(\"\\n--- OpenAI ---\")\n",
    "        result_openai = run_agent(agent_openai, query)\n",
    "        print(\"Final Answer (OpenAI):\", result_openai)\n",
    "        \n",
    "        # Run query with Together AI\n",
    "        print(\"\\n--- Together AI ---\")\n",
    "        result_together = run_agent(agent_together, query)\n",
    "        print(\"Final Answer (Together AI):\", result_together)\n",
    "\n",
    "# Run the comparison\n",
    "compare_providers(queries)\n",
    "\n",
    "# Explanation:\n",
    "# - This section initializes agents for both OpenAI and Together AI using the `initialize_agent_with_provider` function.\n",
    "# - It defines a list of queries to test the agents' responses.\n",
    "# - The `compare_providers` function runs each query with both agents and prints the results for comparison.\n",
    "# - This allows users to analyze differences in reasoning patterns, tool selection behavior, and response quality between OpenAI and Together AI."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
